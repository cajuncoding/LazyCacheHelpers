using System;
using System.Collections.Generic;
using System.Diagnostics;
using System.Linq;
using System.Threading.Tasks;
using System.Runtime.Caching;
using System.Threading;
using Nito.AsyncEx;

namespace LazyCacheHelpers
{
    /// <summary>
    /// BBernard
    /// Original Source (MIT License): https://github.com/cajuncoding/LazyCacheHelpers
    /// 
    /// LazyCache Handler for easy cache implementations at all layers of an application with support for both Sync & Async
    /// Lazy operations to maximize server utilization and performance!
    /// 
    /// This Handler supports changing the underlying Cache Repository with different implementations via ILazyCacheRepository implementation.
    /// 
    /// This class provides a completely ThreadSafe cache with Lazy Loading capabilities in an easy to use implementation that can work at 
    ///     all levels of an application (classes, controllers, etc.).
    ///     
    /// The use of Lazy<T> loading facilitates self-populating cache so that the long running processes are never 
    ///     executed more than once.  Even if they are triggered at the exact same time, no more than one thread will
    ///     ever perform the work, dramatically decreasing server utilization under high load.
    ///     
    /// NOTE: A wrapper implementation for MemoryCache is implemented (via default ICacheRepository implementation as LazyDotNetMemoryCacheRepository) to 
    ///     make working with MemoryCache with greatly simplified support for self-populating (Lazy) initialization.
    /// </summary>
    public class LazyCacheHandler<TValue> : ILazyCacheHandler<TValue>, ILazyCacheHandlerSelfExpiring<TValue>, IDisposable where TValue : class
    {
        //Added methods to CacheHelper to work with MemoryCache more easily.
        //NOTE: .Net MemoryCache supports this does NOT support Garbage Collection and Resource Reclaiming so it should
        //      be used whenever caching dynamic runtime data.
        private readonly ILazyCacheRepository _cacheRepository;

        private readonly AsyncReaderWriterLock _selfExpiringReaderWriterLock = new AsyncReaderWriterLock();

        /// <summary>
        /// Initializes with the specified cacheRepository; if null is specified then DotNetMemoryCacheRepository (leveraging MemoryCache.Default) is used.
        /// </summary>
        public LazyCacheHandler(ILazyCacheRepository cacheRepository)
        {
            _cacheRepository = cacheRepository ?? new LazyDotNetMemoryCacheRepository();
        }

        /// <summary>
        /// Initializes with default DotNetMemoryCacheRepository (leveraging MemoryCache.Default).
        /// </summary>
        public LazyCacheHandler()
            : this(null)
        {}

        /// <summary>
        /// This overload enables dynamic self-populating cache retrieval of the results on any result generated by the specified
        /// cache item/result factory Func&lt;T&gt;. Using the Cache Item Expiration Policy provided the cache will ensure that
        /// the work is only ever performed by one and only one request, whereby all other simultaneous requests will immediately
        /// benefit from the work and receive the cached item once it's generated.
        /// 
        /// In this overload the CacheItem policy is known before executing the logic so it is consistent for all calls to this method.
        /// 
        /// Ultimately this is a wrapper implementation for ICacheRepository to make working with Thread Safety significantly easier.
        /// This provides completely ThreadSafe cache with Lazy Loading capabilities in an easy to use function; 
        /// Lazy<typeparamref name="TValue"/> loading facilitates self-populating cache so that the long running processes are never 
        /// executed more than once, even if they are triggered at approx. the same time.
        /// 
        /// This method handles the Add or Retrieval of an item from Memory Cache using a Lambda initialization function that will
        /// only ever be executed once for the Cache Key provided; by leveraging the .Net Lazy<> initializer. 
        /// 
        /// More information on this Cache design pattern for ease of use can be found here:
        /// https://blog.falafel.com/working-system-runtime-caching-memorycache/
        /// </summary>
        /// <typeparam name="TKey"></typeparam>
        /// <param name="key"></param>
        /// <param name="fnValueFactory"></param>
        /// <param name="cacheItemPolicy"></param>
        /// <returns></returns>
        /// <exception cref="ArgumentNullException"></exception>
        public virtual TValue GetOrAddFromCache<TKey>(TKey key, Func<TValue> fnValueFactory, CacheItemPolicy cacheItemPolicy)
        {
            //We support either ILazyCacheKey interface or any object for the Cache Key as long as it's ToString() 
            //  implementation creates a valid unique Key for us, so here we initialize the Cache Key to use.
            string cacheKey = GenerateCacheKeyHelper(key);

            //We support a lambda initializer function to initialize the Cached Item value if it does not already exist, but we
            //must ensure it only ever run's one time for the Cache Key provided, so we leverage the .Net Lazy<> initializer which
            //guarantees that this code will be Thread Safe and Lazily initialized to only run one time!
            var newValueLazyInitializer = new Lazy<TValue>(fnValueFactory);

            //Using .Net Memory Cache (which supports garbage collection and resource re-claim as needed) we use the Thread safe
            //AddOrGetExisting() method to efficiently initialize and/or retrieve our item from Cache.
            //NOTE: Because Unknown code may run during the Initialization and execution of the Lambda function, we must
            //      provide Error Trapping Here to minimize impact to Cache and propagate the error up to calling code!
            var existingCachedLazyInitializer = _cacheRepository.AddOrGetExisting(cacheKey, newValueLazyInitializer, cacheItemPolicy) as Lazy<TValue>;

            try
            {
                //NOTE: We use the cached existing initializer if it is not null, but fall back to the new one.  
                //      This is very important because it is the existing Cached initializer that will have already executed
                //      the value factory lambda/code and therefore guarantees that our initialization code only runs once.
                //      Only if null do we use the new Lazy<> created that was also just added to the Cache, executing it's 
                //      lambda code for the first and only time!
                return (existingCachedLazyInitializer ?? newValueLazyInitializer).Value;
            }
            catch
            {
                // Handle cached lazy exception by evicting from cache. Thanks to Denis Borovnev for pointing this out!
                _cacheRepository.Remove(cacheKey);
                throw;
            }
        }

        /// <summary>
        /// This overload enables dynamic self-populating cache retrieval whereby the actual cache item logic also returns
        /// the cache expiration policy in addition to the cache item result.  This is very useful in cases such as Auth tokens,
        /// and external API calls whereby the response contains information about how long the data returned is valid. And therefore
        /// the response can be used to construct a highly optimized Cache Expiration Policy based on the data returned -- rather than
        /// simply guessing and/or hard coding how long the data is valid for.
        /// NOTE: There is a small amount of overhead to support this feature due to the need to manually check for existence and
        ///         lock to to generate the result and policy.
        /// </summary>
        /// <typeparam name="TKey"></typeparam>
        /// <param name="key"></param>
        /// <param name="fnValueFactory"></param>
        /// <returns></returns>
        /// <exception cref="ArgumentNullException"></exception>
        public TValue GetOrAddFromCache<TKey>(TKey key, Func<ILazySelfExpiringCacheResult<TValue>> fnValueFactory)
        {
            if (fnValueFactory == null)
                throw new ArgumentNullException(nameof(fnValueFactory));

            ILazySelfExpiringCacheResult<TValue> selfExpiringCacheResult = null;
            TValue cacheResult;

            using (var readLock = _selfExpiringReaderWriterLock.ReaderLock())
            {
                cacheResult = GetOrAddFromCache(key, () =>
                    {
                        //Wrap and Set the captured self-expiring cache result when-and-only-when this value factory executes;
                        //  ensuring that this runs only on initial load when the very first request executes it!
                        selfExpiringCacheResult = fnValueFactory();
                        return selfExpiringCacheResult.CacheItem;
                    },
                    //To ensure that we fully support self-populating cache implementation (so that all other requests get the result of
                    //  the work completed by the very first request we always enforce an Infinite Cache initially, but it will be immediately
                    //  re-initialized with the correct self-expiring policy returned after value factory is initially invoked (below)!
                    //NOTE: Though the only thread that will do this is the initial thread.
                    LazyCachePolicy.InfiniteCachingPolicy
                );
            }

            if (selfExpiringCacheResult != null)
            {
                //Since the Cache Policy is only available after the Cache Value Factory has executed we must now utilize the results
                //  to safely initialize the Cache with the valid Policy!
                //NOTE: There is a risk that while we remove and update the cache item with the correct policy that another request
                //      might attempt to get it from the cache at this same time. Therefore to ensure that we are 100% self-populating we
                //      must initialize the WriteLock to block all Readers while this update occurs!
                //NOTE: This works as designed because the read/write lock will block the write (below) until all simultaneous reads are fulfilled
                //      (which will be fast as the cache fulfills them), and then block further reads while the valid update write is fulfilled to invoke the correct Policy!
                //NOTE: Due to the Reader/Writer lock process this incurs some minimal additional overhead, but ONLY for the initial cache invocation and 
                //      only if the value factory executes, therefore with Reader/Writer lock this is still fully self-populating and
                //      has negligible impact to enforcing support as a self-populating cache (relative to the cost of executing the value factory more than once).
                using (var readLock = _selfExpiringReaderWriterLock.WriterLock())
                {
                    //REMOVE and then Add the already computed Cache Result immediately with the now-known Cache Policy!
                    RemoveFromCache(key);
                    GetOrAddFromCache(key, () => cacheResult, selfExpiringCacheResult.CachePolicy);
                }
            }

            return cacheResult;
        }

        /// <summary>
        /// This overload enables async dynamic self-populating cache retrieval of the results on any result generated by the specified
        /// cache item/result factory Func&lt;T&gt;. Using the Cache Item Expiration Policy provided the cache will ensure that
        /// the work is only ever performed by one and only one request, whereby all other simultaneous requests will immediately
        /// benefit from the work and receive the cached item once it's generated.
        /// 
        /// In this overload the CacheItem policy is known before executing the logic so it is consistent for all calls to this method.
        /// 
        /// Ultimately this is an Async wrapper implementation for using ICacheRepository to make working with Thread Safety for 
        /// Asynchronous processes significantly easier. This provides completely ThreadSafe Async cache with Lazy Loading capabilities 
        /// in an easy to use function; Lazy<typeparamref name="TValue"/> loading facilitates self-populating cache so that the long running processes are never 
        /// executed more than once, even if they are triggered at approx. the same time.
        /// 
        /// This method handles the Asynchronous Add or Retrieval of an item from Memory Cache using a Lambda initialization function that will
        /// only ever be executed once for the Cache Key provided; by leveraging the an AsyncLazy<> initializer. 
        /// 
        /// NOTE: Using .Net MemoryCache via LazyDotNetMemoryCacheRepository we support garbage collection and resource re-claims as needed
        /// NOTE: We also prevent negative caching so exceptions thrown during long running processes are not cached!
        /// 
        /// More information on this Cache design pattern for ease of use can be found here:
        /// https://blog.falafel.com/working-system-runtime-caching-memorycache/
        /// https://cpratt.co/thread-safe-strongly-typed-memory-caching-c-sharp/
        /// 
        /// BBernard - 03/06/2018
        /// NOTE: WE DO NOT IMPLEMENT AsyncLazy() in the same was as the above blogs because it results in the process ALWAYS running on
        ///         background threads using Task.Factory.StartNew (e.g. Task.Run is the best practice), but in our caching we don't
        ///         want to force anything to run on different threads, instead we simply want to ensure that we correctly support
        ///         the Async/Await Task Based Asynchronous pattern from top to bottom and even with our caching!
        /// 
        /// </summary>
        /// <typeparam name="TKey"></typeparam>
        /// <param name="key"></param>
        /// <param name="fnAsyncValueFactory"></param>
        /// <param name="cacheItemPolicy"></param>
        /// <returns></returns>
        /// <exception cref="ArgumentNullException"></exception>
        public virtual async Task<TValue> GetOrAddFromCacheAsync<TKey>(TKey key, Func<Task<TValue>> fnAsyncValueFactory, CacheItemPolicy cacheItemPolicy)
        {
            if (fnAsyncValueFactory == null)
                throw new ArgumentNullException(nameof(fnAsyncValueFactory));

            //We support either ILazyCacheKey interface or any object for the Cache Key as long as it's ToString() 
            //  implementation creates a valid unique Key for us, so here we initialize the Cache Key to use.
            string cacheKey = GenerateCacheKeyHelper(key);

            //BBernard - 03/06/2018
            //We support a lambda initializer function to initialize the Cached Item value if it does not already exist, but we
            //must ensure it only ever run's one time for the Cache Key provided, so we leverage the .Net Lazy<> initializer which
            //guarantees that this code will be Thread Safe and Lazily initialized to only run one time!
            var newValueAsyncLazyInitializer = new Lazy<Task<TValue>>(fnAsyncValueFactory);

            //BBernard - 03/06/2018
            //Using .Net Memory Cache (which supports garbage collection and resource re-claim as needed) we use the Thread safe
            //AddOrGetExisting() method to efficiently initialize and/or retrieve our item from Cache.
            //NOTE: Because Unknown code may run during the Initialization and execution of the Lambda function, we must
            //      provide Error Trapping Here to minimize impact to Cache and propagate the error up to calling code!
            var existingCachedAsyncLazyInitializer = _cacheRepository.AddOrGetExisting(cacheKey, newValueAsyncLazyInitializer, cacheItemPolicy) as Lazy<Task<TValue>>;
            try
            {
                //BBernard - 03/06/2018
                //NOTE: We use the cached existing initializer if it is not null, but fall back to the new one.  
                //      This is very important because it is the existing Cached initializer that will have already executed
                //      the value factory lambda/code and therefore guarantees that our initialization code only runs once.
                //      Only if null do we use the new Lazy<> created that was also just added to the Cache, executing it's 
                //      lambda code for the first and only time!
                Task<TValue> asyncResultTask = (existingCachedAsyncLazyInitializer ?? newValueAsyncLazyInitializer).Value;

                //BBernard - 03/06/2018
                //NOTE: Since this is an Async process we need to validate that the Task<T> is not faulted or in an exception state
                //      because we do NOT allow negative caching (e.g. caching of failed results).
                if (asyncResultTask == null || asyncResultTask.IsFaulted || asyncResultTask.IsCanceled)
                {
                    _cacheRepository.Remove(cacheKey);
                }

                //BBernard - 03/06/2018
                //FINALLY we await the actual Task before returning, this ensures that the Async/Await pattern is correctly
                //followed, allowing the Task to execute before continuing, which ensures that our Try/Catch block error handling
                //is fully supported for Async Tasks!
                return await asyncResultTask;
            }
            catch
            {
                //BBernard
                //No Exceptions should be cached, so we handle cached lazy exceptions by evicting from cache!
                _cacheRepository.Remove(cacheKey);
                throw;
            }
        }

        /// <summary>
        /// This overload enables async dynamic self-populating cache retrieval whereby the actual cache item logic also returns
        /// the cache expiration policy in addition to the cache item result.  This is very useful in cases such as Auth tokens,
        /// and external API calls whereby the response contains information about how long the data returned is valid. And therefore
        /// the response can be used to construct a highly optimized Cache Expiration Policy based on the data returned -- rather than
        /// simply guessing and/or hard coding how long the data is valid for.
        /// </summary>
        /// <typeparam name="TKey"></typeparam>
        /// <param name="key"></param>
        /// <param name="fnAsyncValueFactory"></param>
        /// <returns></returns>
        /// <exception cref="ArgumentNullException"></exception>
        public async Task<TValue> GetOrAddFromCacheAsync<TKey>(TKey key, Func<Task<ILazySelfExpiringCacheResult<TValue>>> fnAsyncValueFactory)
        {
            if (fnAsyncValueFactory == null)
                throw new ArgumentNullException(nameof(fnAsyncValueFactory));

            ILazySelfExpiringCacheResult<TValue> selfExpiringCacheResult = null;
            TValue cacheResult;

            using (var readLock = await _selfExpiringReaderWriterLock.ReaderLockAsync())
            {
                cacheResult = await GetOrAddFromCacheAsync(key, async () =>
                    {
                        //Wrap and Set the captured self-expiring cache result when-and-only-when this value factory executes;
                        //  ensuring that this runs only on initial load when the very first request executes it!
                        selfExpiringCacheResult = await fnAsyncValueFactory();
                        return selfExpiringCacheResult.CacheItem;
                    },
                    //To ensure that the value is not cached we specify DisableCachePolicy so that it can be re-initialized with the
                    //  self-expiring policy returned after value factory is initially invoked (below)!
                    LazyCachePolicy.InfiniteCachingPolicy
                );
            }

            //NOTE: Since the Lazy cache already enforces that one-and-only-one value factory will execute this is optimized
            //      to only execute on the thread single request that executed the value factory...
            if (selfExpiringCacheResult != null)
            {
                //Since the Cache Policy is only available after the Cache Value Factory has executed we must now utilize the results
                //  to safely initialize the Cache with the valid Policy!
                //NOTE: There is a risk that while we remove and update the cache item with the correct policy that another request
                //      might attempt to get it from the cache at this same time. Therefore to ensure that we are 100% self-populating we
                //      must initialize the WriteLock to block all Readers while this update occurs!
                //NOTE: This works as designed because the read/write lock will block the write (below) until all simultaneous reads are fulfilled
                //      (which will be fast as the cache fulfills them), and then block further reads while the valid update write is fulfilled to invoke the correct Policy!
                //NOTE: Due to the Reader/Writer lock process this incurs some minimal additional overhead, but ONLY for the initial cache invocation and 
                //      only if the value factory executes, therefore with Reader/Writer lock this is still fully self-populating and
                //      has negligible impact to enforcing support as a self-populating cache (relative to the cost of executing the value factory more than once).
                using (var readLock = await _selfExpiringReaderWriterLock.WriterLockAsync())
                {
                    //REMOVE and then Add the already computed Cache Result immediately with the now-known Cache Policy!
                    RemoveFromCache(key);
                    await GetOrAddFromCacheAsync(key, () => Task.FromResult(cacheResult), selfExpiringCacheResult.CachePolicy);
                }
            }

            return cacheResult;
        }

        /// <summary>
        /// BBernard
        /// 
        /// Remove the item corresponding to the specified Cache Key from the Cache.
        /// </summary>
        /// <typeparam name="TKey"></typeparam>
        /// <param name="key"></param>
        public virtual void RemoveFromCache<TKey>(TKey key)
        {
            //We support either ILazyCacheKey interface or any object for the Cache Key as long as it's ToString() 
            //  implementation creates a valid unique Key for us, so here we initialize the Cache Key to use.
            var cacheKey = GenerateCacheKeyHelper(key);

            //Remove the Item from the underlying Cache Repository
            _cacheRepository.Remove(cacheKey);
        }

        /// <summary>
        /// BBernard
        /// Clear/Purge all entries from the underlying Cache Repository.
        /// </summary>
        public virtual void ClearEntireCache()
        {
           _cacheRepository.ClearAll();
        }

        public long CacheEntryCount()
        {
            return _cacheRepository.CacheEntryCount();
        }

        #region Private Helpers

        /// <summary>
        /// Dynamically determine the Cache Key from the specified object that implements either 
        /// ILazyCacheKey or provides a ToString() method that renders a valid Cache Key.
        /// </summary>
        /// <typeparam name="TKey"></typeparam>
        /// <param name="cacheKeyGenerator"></param>
        /// <returns></returns>
        protected virtual string GenerateCacheKeyHelper<TKey>(TKey cacheKeyGenerator)
        {
            //If Null then throw Argument Exception...
            if (cacheKeyGenerator == null) throw new ArgumentNullException(
                nameof(cacheKeyGenerator), 
                $"The cache key object is null; a valid object that implements either {nameof(ILazyCacheKey)} or ToString() override must be specified."
            );

            var generatedKey = (cacheKeyGenerator as ILazyCacheKey)?.GenerateKey() ?? cacheKeyGenerator.ToString();
            return generatedKey;
        }

        #endregion

        #region IDisposable Implementation

        protected virtual void Dispose(bool disposing)
        {
            if (disposing)
            {
                this.ClearEntireCache();
                if(_cacheRepository is IDisposable disposableRepository)
                    disposableRepository.Dispose();
            }
        }

        public void Dispose()
        {
            Dispose(true);
            GC.SuppressFinalize(this);
        }

        #endregion
    }
}
